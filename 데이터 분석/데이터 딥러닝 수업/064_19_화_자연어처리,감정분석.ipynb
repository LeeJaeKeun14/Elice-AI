{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 \n",
    "\n",
    "## 자연어 처리란\n",
    "\n",
    "- 자연어 처리 (Natural Language Processing, NLP)는 컴퓨터를 톤해 인간의 언어를 분석 및 처리하느 인공지능의 한 분야\n",
    "\n",
    "- 자연어 처리의 적용 사례\n",
    "  - 문서 분류\n",
    "  - 키워드 추출\n",
    "  - 감정 분석\n",
    "\n",
    "## 모델링을 위한 데이터 탐색 및 전처리\n",
    "\n",
    "- 일반 데이터\n",
    "  - 데이터 탐색\n",
    "    - 데이터 통계치\n",
    "    - 변수별 특징\n",
    "  - 데이터 전처리\n",
    "    - 이상치 제거\n",
    "    - 정규화(normalization)\n",
    "\n",
    "- text 데이터\n",
    "  - 데이터 탐색\n",
    "    - 단어의 개수\n",
    "    - 단어별 빈도수\n",
    "  - 데이터 전처리\n",
    "    - 특수기호 제거\n",
    "    - 단어 정규화\n",
    "\n",
    "### 토큰화\n",
    "\n",
    "- Tokenization\n",
    "  - 토큰화는 주어진 텍스트를 각 단어 기준으로 분리하는것을 의미\n",
    "    - 기본적으로 띄어쓰기, ?, .. ! 등으로 구분\n",
    "    - 영문일 경우 소문자 처리 및 특수기호 제거\n",
    "\n",
    "```python\n",
    "counter = dict()\n",
    "with open(파일명, 'r') as f:\n",
    "    for line in f:\n",
    "        for word in line.rstrip().split():\n",
    "            if word not in word_counter:\n",
    "                word_counter[word] = 1\n",
    "            else:\n",
    "                word_counter[word] += 1\n",
    "```\n",
    "\n",
    "- 자연어 처리 + 머신러닝\n",
    "- 대부분 단어 빈도수의 분포는 지프의 법칙(Zipf's  law)을 따름\n",
    "\n",
    "### 전처리\n",
    "\n",
    "- 전처리 1 : 특수기호 제거\n",
    "\n",
    "- re\n",
    "  - 정규표현식: 정의하는 규칙을 가진 문자열 집합\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "word = \"...\"\n",
    "regex = re.compile('[^a-z A-Z]')\n",
    "print(regex.sub('', word))\n",
    "```\n",
    "\n",
    "- 전처리 2 : Stopword 제거\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = [\"the\", \"green\", \"egg\", \"and\", \"ham\", \"a\", \"an\"]\n",
    "stopwords = stopwords.words('english')\n",
    "new_sentence = [word for word in sentence if word not in stopwords]\n",
    "```\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "new_stopwords = [\"none\", \"는\", \"가\"]\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "stopwords += new_stopwords\n",
    "```\n",
    "\n",
    "- 전처리 3 : Stemming\n",
    "  - 동일한 의미의 단어이지만, 문법적인 이유 등 표현 방식이 다양한 단어를 공통된 형태로 변환\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"studies\", \"studied\", \"studying\", \"dogs\", \"dog\"]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))\n",
    "# studi, studi, studi, dog, dog\n",
    "```\n",
    "\n",
    "## 단어 임베딩\n",
    "\n",
    "- 컴퓨터 언어\n",
    "  - 컴퓨터는 텍스트를 포함하여 모든 데이터를 0과 1로 처리\n",
    "  - 장연어의 기본 단위인 단어를 수치형 데이터로 표현하는 것이 중요\n",
    "\n",
    "- 단어 임베딩\n",
    "  - 각 단어를 연속형 벡터로 표현하는 방법을 의미\n",
    "  - 비슷한 문맥에서 발생하는 단어는 유사한 의미를 지님\n",
    "  - 유사한 단어의 임베딩 벡터는 인접한 공간에 위치\n",
    "\n",
    "## word2vec\n",
    "\n",
    "- 주어진 문맥에서 발생하는 단어를 예측하는 문제로 통해 단어 임베딩 벡터를 학습\n",
    "  - 각 단어의 벡터는 해당 단어가 입력으로 주어졌을 때 계산되는 은닉층의 값을 사용\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "doc = [\"서울에 살고 있는 엘리스는 강아지를 좋아한다\".split()]\n",
    "\n",
    "w2v_model = Word2Vec(min_count = 1, window = 2, vector_size = 300)\n",
    "w2v_model.build_vocab(doc)\n",
    "w2v_model.train(doc, total_examples=w2v_model.corpus_count, epochs=20)\n",
    "```\n",
    "\n",
    "```python\n",
    "similar_word = w2v_model.wv.most_similar(\"엘리스는\")\n",
    "print(similar_word)\n",
    "\n",
    "score = w2v_model.wv.similarity(\"엘리스는\", \"좋아한다\")\n",
    "print(score)\n",
    "```\n",
    "\n",
    "## fastText\n",
    "\n",
    "- 학습 데이터 내 존재하지 않았던 단어 벡터는 생성할 수 없음\n",
    "  - 미등록 단어 문제, out-of-vocabulary\n",
    "  - 각 단어를 문자단위로 나누어서 단어 임베딩 벡터를 학습\n",
    "\n",
    "- fastText\n",
    "  - 학습 데이터에 존재하지 않았던 단어의 임베딩 벡터 또한 생성 가능\n",
    "\n",
    "```python\n",
    "from gensim.models import FastText\n",
    "\n",
    "doc = [\"서울에 살고 있는 엘리스는 강아지를 좋아한다\".split()]\n",
    "\n",
    "ft_model = FastText(min_count=1, window=2, vector_sim=300)\n",
    "ft_model.build_vocab(doc)\n",
    "ft_model.train(doc, total_examples=ft_model.corpus_count, epochs=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정 분석\n",
    "\n",
    "- 뉴스, 백과 사전 같은 텍스트는 객관적인 정보를 제공\n",
    "- 리뷰, 소설 같은 텍스트는 저자의 주관적인 평가나 감정을 표현\n",
    "\n",
    "- 대량의 텍스트가 있는 경우, 일일이 데이터를 하나씩 살펴보고 판단하기 어려움\n",
    "- 비슷한 감정을 표현하는 문서는 유사한 단어 구성 및 언어적 특징을 보일 것을 가정\n",
    "\n",
    "## 감정 분석이란\n",
    "\n",
    "- 텍스트 내 감정을 분류하거나 긍정/부정의 정도를 점수화\n",
    "\n",
    "- 감정 분석 서비스\n",
    "  - 머신러닝 기반 감정 분석 서비스의 경우 데이터를 통한 모델 학습 부터 시작\n",
    "  - 학습된 머신러닝 모델을 통해 신규 텍스트의 감정을 예측\n",
    "\n",
    "## 나이브 베이즈의 원리\n",
    "\n",
    "- 나이브 베이즈 기반 감정 분석은 주어진 텍스트가 특정 감정을 나타낼 확률을 예측하는 문제로 정의\n",
    "- 베이즈 정리를 사용하여 텍스트의 감정을 발생 확률을 추정\n",
    "\n",
    "- 감정의 발생 확률과 텍스트를 구성하는 단어들의 가능도(likelihood)로 텍스트의 감정을 예측\n",
    "\n",
    "### 단어의 가능도\n",
    "\n",
    "- 텍스트 데이터에서는 가능도는 단어의 빈도수로 추정\n",
    "- 감정의 발생 확률\n",
    "  - 감정의 발생 확률은 주어진 텍스트 데이터 내 해당 감정을 표현하는 문서의 비율로 추정\n",
    "\n",
    "### 텍스트의 감정\n",
    "\n",
    "- 텍스트의 감정별 확률값 중 최대 확률값을 나타내는 감정을 해당 문서의 감정으로 예측\n",
    "\n",
    "## 나이브 베이즈 기반 감정 예측\n",
    "\n",
    "- 스무딩(smoothing)\n",
    "  - 학습 데이터 내 존재하지 않은 단어의 빈도수를 보정\n",
    "  - 단어의 감정별 가능도와 감정의 발생 확률을 모구 소수로 표현\n",
    "\n",
    "- 나이브 베이즈\n",
    "  - 로그 확률값의 합으로 텍스트의 감정을 예측\n",
    "\n",
    "## scikit-learn 을 통한 나이브 베이즈 구현\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CounterVectorizer\n",
    "\n",
    "doc = [\"i am very happy\", \"this product is really great\"]\n",
    "emotion = ['happy', 'excited']\n",
    "\n",
    "cv = CoutVectorizer()\n",
    "csr_doc_matcix = cv.fit_transform(doc)\n",
    "\n",
    "print(csr_doc_matcix)\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "doc = [\"i am very happy\", \"this product is really great\"]\n",
    "emotion = ['happy', 'excited']\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(csr_doc_matcix, emotion)\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CounterVectorizer\n",
    "\n",
    "doc = [\"i am very happy\", \"this product is really great\"]\n",
    "emotion = ['happy', 'excited']\n",
    "\n",
    "cv = CoutVectorizer()\n",
    "csr_doc_matcix = cv.fit_transform(doc)\n",
    "\n",
    "clf.fit(csr_doc_matcix)\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "test_doc = [\"i am really great\"]\n",
    "\n",
    "transformed_test = cv.transform(test_doc)\n",
    "pred = clf.predict(transformed_test)\n",
    "\n",
    "print(pred)\n",
    "```\n",
    "\n",
    "- 감정 분석 + 머신러닝\n",
    "\n",
    "- 임베딩 벡터를 사용하여, 머신러닝 알고리즘 적용이 가능\n",
    "  - 가장 간단한 방법으로 단어 임베딩 벡터의 평균을 사용\n",
    "\n",
    "- CNN\n",
    "  - 단어 임베딩 벡터에 필터를 적용하여 CNN기반으로 감정 분류\n",
    "- RNN\n",
    "  - 문자 단위로 단어를 분리하여 RNN 기반으로 분류 및 여측\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
