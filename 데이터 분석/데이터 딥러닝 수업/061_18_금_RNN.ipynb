{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[어텐션 메커니즘](https://wikidocs.net/22893)\\\n",
    "[순환 신경망](https://wikidocs.net/22886)\n",
    "\n",
    "- 택스트 분석, 순환신경망 자료\n",
    "- 형태소 분석기 Mecab???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "## 순차 데이터란\n",
    "\n",
    "- 왜 RNN과 순차 데이터인가?\n",
    "  - RNN은 CNN과 함께 대표적인 딥러닝 모델\n",
    "  - 시계열 데이터 같은 순차 데이터(Sequential Data)처리를 위한 모델\n",
    "  - RNN의 이해에는 순차 데이터가 가지는 특징의 이해가 필요\n",
    "\n",
    "- 순차 데이터\n",
    "  - 순서(Order)를 가지고 나타나는 데이터\n",
    "  - 날짜에 따른 기온 데이터나 단어들로 이루어진 문장 등으로 유명\n",
    "    - 데이터 내 각 객체 간의 순서가 중요\n",
    "\n",
    "- 자연어 데이터 (Natural Language)\n",
    "  - 인류가 말하는 언어를 의미\n",
    "  - 주로 문장 내에서 단어가 등장하는 순서에 주목\n",
    "\n",
    "## 딥러닝을 활용한 순차 데이터 처리\n",
    "\n",
    "- 경향성 파악\n",
    "  - 주가 예측\n",
    "  - 기온 예측\n",
    "\n",
    "## 음악 장르 분석\n",
    "\n",
    "- 오디오 파일은 본질적으로 시계열 데이터\n",
    "- 음파 형태 등을 분석하여 오디오 파일의 장르를 분석\n",
    "\n",
    "- 강수량 예측\n",
    "\n",
    "- 음성 인식\n",
    "\n",
    "- 번역기\n",
    "- 챗봇\n",
    "\n",
    "## Recurrent Neural Network\n",
    "\n",
    "- Fully connected Layer 와 순차 데이터\n",
    "\n",
    "- FC Layer는 입력 노드 개수와 출력 노드 개수가 정해짐\n",
    "- 순차 데이터는 하나의 데이터를 이루는 개체 수가 다를 수 있음\n",
    "  - 문장은 모두 서로 다른 개수의 단어로 이루어짐\n",
    "- 또한 FC Layer는 순서 고려가 불가능\n",
    "\n",
    "### RNN\n",
    "\n",
    "- 따라서 순차 데이터 처리를 위한 딥러닝 모델이 등장\n",
    "- RNN의 대표적인 구성 요소\n",
    "  - Hidden State: 순환 구조를 구현하는 핵심 장치\n",
    "\n",
    "- 입력 데이터 구조\n",
    "  - 시계열 데이터 : 일정 시간 간격으로 나눠진 데이터 개체 하나\n",
    "  - 자연어 데이터 : 문장 내의 각 단어\n",
    "\n",
    "- 시계열 데이터의 벡터 변환\n",
    "  - 입력 데이터의 각 x는 벡터 형태\n",
    "  - 시계열 데이터의 경우 각 데이터를 이루는 Feature값들을 원소로 하여 벡터로 변환\n",
    "\n",
    "- 자연어 데이터의 벡터 변환\n",
    "  - 임베딩(Embedding): 각 단어들을 숫자로 이루어진 벡터로 변환\n",
    "  - 대표적인 임베딩 기법\n",
    "    - One-hot Encoding\n",
    "    - Word2Vec : 주어진 단어들을 벡터로 변환하는 기계학습 모델\n",
    "\n",
    "## Vanilla RNN\n",
    "\n",
    "- 가장 간단한 형태의 RNN 모델: Simple RNN이라고도 불림\n",
    "\n",
    "![](image/061_001.png)\n",
    "\n",
    "- 내부에 세개의 FC Layer로 구성\n",
    "  - 현재 입력값 x에 대한 새로운 hidden state계산\n",
    "  - h1 = hanh(h_t-1 * W_hh + x_t * W_xh)\n",
    "- 현재 입력값x에 대한 새로운 출력값 계산\n",
    "- y = Wh\n",
    "  - 앞서 계산한 hidden state를 이용\n",
    "\n",
    "### 시간순으로 보는 Vanilla RNN의 연산 과정\n",
    "\n",
    "- 모델에 들어오는 각 시점의 데이터 x마다 앞서 설명한 연산 과정을 수행\n",
    "- 입력값에 따라 반복해서 출력값과 hidden state를 계산\n",
    "- 이전 시점에 생성된 hidden state를 다음 시점에 사용\n",
    "\n",
    "### Vanilla RNN 의의\n",
    "\n",
    "- Hidden state의 의미\n",
    "  - 특정 시점 t까지 들어온 입력값들의 상관관계나 경향성 정보를 압축해서 저장\n",
    "  - 모델이 내부적으로 계속 가지는 값이므로 일종의 메모리(Memory)로 볼 수 있음\n",
    "  - 컴퓨터의 메모리와 일맥상통\n",
    "\n",
    "- Parameter Sharing\n",
    "  - Hidden state와 출력값 계산을 위한 FC Layer를 모든 시점의 입력값이 재사용\n",
    "  - FC Layer 세 개가 모델 파라미터의 전부 \n",
    "\n",
    "### Vanilla RNN의 종류\n",
    "\n",
    "- 사용할 입력값과 출력값의 구성에 따라 여러 종류의 RNN이 존재\n",
    "  - 다대일(many-to-one): 출력값을 한 시점의 값만 사용\n",
    "  - 다대다(many-to-many): 여러 시점의 입력값과 여러 시점의 출력값을 사용\n",
    "    - 입력값과 출력값에 사용하는 시점의 개수는 같을 수도 있고 다를 수도 있음\n",
    "\n",
    "- 인코더 - 디코더 : 입력값들을 받아 특정 hidden state로 인코딩한 후, 이 hidden state로 새로운 출력값을 만드는 구조\n",
    "\n",
    "### Vanilla RNN의 문제점\n",
    "\n",
    "- RNN은 출력값이 시간 순서에 따라 생성\n",
    "- 각 시점의 출력값과 실제값을 비교하여 손실(Loss)값을 계산\n",
    "- 역전파 알고리즘이 시간에 따라 작동 - Back-Propagation Throuhg Time (BPTT)\n",
    "\n",
    "- 입력값이 길이가 매우 길어질 경우\n",
    "  - 초기 입력값과 나중 출력값 사이에 전파되는 기울기가 값이 매우 작아질 가능성이 높음\n",
    "- 기울기 소실(Vanishing Gradient) 문제가 발생하기 쉬움\n",
    "  - 다른 말로 장기 의존성(Long-term Dependency)을 다루기가 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리를 위한 문자열 인코딩\n",
    "\n",
    "자연어를 모델에 입력하기 위해서는 숫자로 변환하는 과정이 필요합니다.\n",
    "\n",
    "이번 실습에서는 one-hot 인코딩을 구현해보고 단점들을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer는 파이썬에서 제공하는 텍스트 데이터 전처리 모듈입니다.\n",
    "tokenizer를 사용하면 문자열로 구성된 데이터를 다양한 형태로로 토큰화할 수 있습니다.\n",
    "\n",
    "set_token함수는 문자열들을 입력받아 그에 맞는 tokenizer 반환하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_token(texts):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text2seq 함수와 seq2onehot는 하나의 문자열을 시퀀스 정보로 변환하고 one-hot 인코딩하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2seq(text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "def seq2onehot(seq, num_word):\n",
    "    return to_categorical(seq,num_classes=num_word+1) # 예약된 토큰을 위해 1자리를 추가로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 문자열들을 처리하는 Tokenizer를 만들고, 텍스트를 one-hot 인코딩하는 과정을 실습해보겠습니다.\n",
    "\n",
    "우선 간단한 영어문장 2개를 준비했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= \"stand on the shoulders of giants\"\n",
    "text2= \"I can stand on mountains\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 두 문장을 위한 Tokenizer를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = set_token([text1,text2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer는 text1과 text2에 있는 단어들을 토큰화할 수 있는 다양한 정보들을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 수:  9\n",
      "단어 인덱스:  {'stand': 1, 'on': 2, 'the': 3, 'shoulders': 4, 'of': 5, 'giants': 6, 'i': 7, 'can': 8, 'mountains': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 수: \", len(tokenizer.word_index))\n",
    "print(\"단어 인덱스: \", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 문장을 합쳐서 9개의 단어가 존재하고 이 단어들이 각각 1~9까지의 정수로 매핑되어 있는 것을 확인하실 수 있습니다.\n",
    "\n",
    "이제 이 tokenizer를 통해 첫번째 문자열인 text2를 정수로 구성된 시퀀스 데이터로 변환하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 1, 2, 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq = text2seq(text2, tokenizer)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개의 단어로 구성된 text2가 단어의 인덱스로 구성된 데이터로 변환되었습니다.\n",
    "\n",
    "RNN모델에 이 데이터를 입력하기 위해서 one-hot 인코딩을 수행하겠습니다.\n",
    "\n",
    "이 과정은 keras에서 제공하는 to_categorical 함수를 이용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "한 단어를 표현하는 길이: 10\n"
     ]
    }
   ],
   "source": [
    "onehot1=seq2onehot(seq, len(tokenizer.word_index))\n",
    "print(onehot1)\n",
    "print(\"한 단어를 표현하는 길이:\", len(onehot1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어가 9가지를 나타내기 위한 9자리와 추후 모델에 입력하기 위해 남겨둔 1자리를 포함해서 10자리를 사용하고 있습니다.\n",
    "\n",
    "상대적으로 간단한 문장들은 문제가 없어보입니다.\n",
    "\n",
    "이번에는 좀 더 긴 문장들을 추가해보겠습니다.\n",
    "\n",
    "아래의 `text3`과 `text4`는 추후에 다뤄볼 IMDB에 존재하는 문장의 일부입니다.\n",
    "\n",
    "이 두 문장과 앞에서 사용했던 `text1``text2`를 모두 합쳐서 같은 과정을 수행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 수:  69\n",
      "단어 인덱스:  {'the': 1, 'of': 2, 'on': 3, 'i': 4, 'and': 5, 'old': 6, 'stand': 7, 'this': 8, 'for': 9, 'that': 10, 'we': 11, 'to': 12, 'in': 13, 'shoulders': 14, 'giants': 15, 'can': 16, 'mountains': 17, 'have': 18, 'copy': 19, 'vhs': 20, 'think': 21, 'they': 22, 'television': 23, 'networks': 24, 'should': 25, 'play': 26, 'every': 27, 'year': 28, 'next': 29, 'twenty': 30, 'years': 31, 'so': 32, \"don't\": 33, 'forget': 34, 'what': 35, 'was': 36, 'remember': 37, 'not': 38, 'do': 39, 'same': 40, 'mistakes': 41, 'again': 42, 'like': 43, 'putting': 44, 'some': 45, 'people': 46, 'he': 47, 'neighborhood': 48, 'serving': 49, 'time': 50, 'an': 51, 'all': 52, 'nice': 53, 'crime': 54, 'necessity': 55, 'course': 56, 'john': 57, 'heads': 58, 'back': 59, 'onto': 60, 'street': 61, 'is': 62, 'greeted': 63, 'by': 64, 'kids': 65, 'dogs': 66, 'ladies': 67, 'his': 68, 'peer': 69}\n"
     ]
    }
   ],
   "source": [
    "text3 = \"i have copy of this on vhs i think they the television networks should play this every year for the next twenty years so that we don't forget what was and that we remember not to do the same mistakes again like putting some people in the\"\n",
    "text4 = \"he old neighborhood in serving time for an all to nice crime of necessity of course john heads back onto the old street and is greeted by kids dogs old ladies and his peer\"\n",
    "\n",
    "tokenizer2 = set_token([text1, text2, text3, text4])\n",
    "\n",
    "print(\"단어 수: \", len(tokenizer2.word_index))\n",
    "print(\"단어 인덱스: \", tokenizer2.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 문장만 추가로 가져왔음에도 불구하고 이번에는 단어가 69가지로 훨씬 많아졌습니다. \n",
    "\n",
    "앞에서 다뤄본 `text2`를 다시 one-hot 인코딩까지 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can stand on mountains\n",
      "[4, 16, 7, 3, 17]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "seq2 = text2seq(text2, tokenizer2)\n",
    "print(text2)\n",
    "print(seq2)\n",
    "onehot2 = seq2onehot(seq2,len(tokenizer2.word_index))\n",
    "print(onehot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 문장이지만 훨씬 길게 표현되는 것을 확인하실 수 있습니다.\n",
    "\n",
    "\n",
    "첫번째 단어인 `I`를 표현하는 벡터를 한번 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(onehot2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `I` 한글자를 위해 70차원의 벡터가 필요합니다.\n",
    "\n",
    "또한, 70개의 숫자중 1개는 `1`이지만, 나머지 69개는 `0`으로 낭비되고 있습니다.\n",
    "\n",
    "\n",
    "하지만 실제 자연어처리를 위한 데이터 셋은 더욱 다양한 단어를 포함하고 있고, one-hot 인코딩은 데이터 셋 전체에 존재하는 모든 단어의 종류만큼 길어질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a72db88ba1b83b14b878414906427eaf31b62ee068e6a91eed760b39b4619b71"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
