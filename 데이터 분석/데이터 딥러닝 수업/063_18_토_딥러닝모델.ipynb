{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습\n",
    "\n",
    "## Background\n",
    "\n",
    "- 텐서플로우\n",
    "  - Google 개발\n",
    "  - 사용이 복잡했으나 2.0이후 개선\n",
    "  - colab연동 쉬움\n",
    "  - Tensorboard를 통한 강력한 시각화\n",
    "\n",
    "- 파이 토치\n",
    "  - Facebook 개발\n",
    "  - 일부 모델의 구현이나 데이터 처리에 더 적합\n",
    "  - 모델의 자세한 구조나 연산을 연구하는 용도로 적합\n",
    "\n",
    "- Tensorflow 2.7\n",
    "  - keras 기능들이 안정적으로 포함됨\n",
    "  - 다양한 구조의 모델을 간편하게 구현 가능하고 사용자 정의 구조를 정의가 개선됨\n",
    "  - GPU를 사용할 수 있는 환경에서 자동으로 데이터를 GPU에 업로드 하는 과정을 수행\n",
    "  - LSTM을 포함한 구조의 연산을 최적화함\n",
    "\n",
    "- 과적합이란 무엇인가\n",
    "  - 모델이 데이터의 일반적인 특징이 아닌 학습 데이터에 특화되어 학습\n",
    "  - 과적합의 원인\n",
    "    1. 데이터의 수나 다양성이 부족하다.\n",
    "    2. 데이터에 비해 모델이 너무 크다.\n",
    "    3. 너무 많은 epoch을 학습했다.\n",
    "  - 해결방안\n",
    "    1. 데이터를 늘린다.\n",
    "    2. 과적합이 일어나기 전에 학습을 정지한다(Early stop)\n",
    "      - 검증 성능이 가장 높은 시접에서 학습을 중단하고 모델을 사용\n",
    "    3. 데이터 증강(Data augmentation)\n",
    "      - 같은 데이터도 반전시키거나 변형해서 입력\n",
    "      - 적은 수의 데이터를 이용하여 좀더 일반적인 특징을 학습하도록 유도\n",
    "\n",
    "### fit 함수\n",
    "\n",
    "```python\n",
    "fit(\n",
    "  x=None, y=None, batch_size=None, epochs=1, verbose='auto',\n",
    "  callbacks=None, validation_split=0.0, validation_data=None,\n",
    "  suffle=True, class_weight=None, initial_epoch=0, \n",
    "  steps_per_spoch=None, validation_steps=None, \n",
    "  validation_batch_size=None, validation_freq=1,\n",
    "  max_queue_size=10, workers=1, use_muliprocessing=False\n",
    ")\n",
    "```\n",
    "- 모델을 학습하는 핵심함수\n",
    "  - 다양한 옵션을 제공하는 인수를 포함하고 있음\n",
    "  - 모델 학습을 위해서는 각 인수의 의미를 파악할 필요가 있음\n",
    "  - 이전 버전에서 사용하던 git_generator 함수도 포함되었음\n",
    "\n",
    "- 학습 데이터\n",
    "  - x : 학습 세트의 입력 데이터\n",
    "  - y : Label, Ground truth, 학습 세트의 입력 데이터를 넣었을 때 정답 값\n",
    "\n",
    "- 학습 진행도를 출력하는 방법\n",
    "  - verbose: 학습 과정을 출력하는 방법\n",
    "    - 0: 출력하지 않음\n",
    "    - 1: 진행바를 표시하여 진행상황을 표시\n",
    "    - 2: 진행바 생략, 수치정보만 표시\n",
    "\n",
    "- Epoch과 batch\n",
    "  - epochs: 학습 데이터를 몇 번 학습할 것인지\n",
    "  - batch_size : 한번에 입력할 데이터 수\n",
    "    - 모델의 input_shape도 이 값을 고려하여 설정\n",
    "  - steps_per_epoch: 한 epoch은 몇 번의 입력을 수행하는지\n",
    "    - None으로 전달하면 batch_size에 따라 자동으로 설정됨\n",
    "    - batch_size * steps_per_epoch = 학습 데이터 수\n",
    "\n",
    "- 학습 데이터 순서를 섞기\n",
    "  - shuffle : 학습데이터를 섞을지 여부\n",
    "    - 정해진 학습 데이터의 입력 순서에 맞춰 학습되지 않도록 순서를 변경\n",
    "    - Generator를 사용할 경우 옵션에 관련 내용이 이미 존재하므로 무시됨\n",
    "\n",
    "- 검증 데이터\n",
    "  - validation_data : 검증 데이터 셋\n",
    "    - (x_val, y_val)의 형태로 전달하거나 Generator로 전달\n",
    "    - 이 데이터는 학습에 반영하지 않고 현재 epoch의 성능을 평가하기 위해 사용\n",
    "  - validation_freq : 검증하는 주기\n",
    "    - 1(기본값) : 매 epoch마다 검증\n",
    "    - 자주 수행할수록 걸리는 시간은 증가하지만, 세밀하게 성능을 평가할 수 있음\n",
    "\n",
    "\n",
    "- 검증 과정의 epoch, batch\n",
    "  - validation_batch_size: 한번 입력에 입력할 데이터의 수\n",
    "  - validation_steps : 한번 검증할 때 데이터 입력의 횟수\n",
    "  - validation_batch_size * validation_steps : 검증 데이터의 수\n",
    "\n",
    "- 데이터 불균형을 위한 가중치\n",
    "  - class_weight: 클래스별 반영 정도\n",
    "    - 데이터 불균형 해결하는 방법 중 하나\n",
    "    - 적은 수의 클래스에는 더 많은 관심을 기울이는 개념\n",
    "    - 클래스별 데이터의 수가 차이날 경우, 적은 수의 클래스는 더 많이 반영하여 학습\n",
    "    - {클래스index: 반영비율}의 딕셔너리 문제를 형태로 전달\n",
    "      - 예) 개(0) 100 마리, 고양이(1) 50마리\n",
    "      - {0:0.5, 1:1.0}\n",
    "\n",
    "- 데이터 병렬 처리\n",
    "  - workers, use_multiprocessing : 병렬 처리를 위한 인수들\n",
    "  - GPU를 사용할 경우 데이터를 읽고 변환하는 과정이 학습하는 시간보다 길게 소모함GPU는 그동안 기다리는 idle time이 발생하면서 효율이 저하\n",
    "  - 여러 Workers가 동시에 데이터를 불러오고 학습을 진행\n",
    "  - Generator를 사용하는 경우에만 사용\n",
    "\n",
    "- 콜백함수의 리스트\n",
    "  - Callback function\n",
    "    - 다른 코드의 인수로 함수를 넘겨주면, 그 코드가 필요에 따라 실행하는 함수\n",
    "    - callbacks 옵션으로 함수들의 리스트를 전달하면 학습 과정 중에 Tensorflow 가 실행\n",
    "    - 다양한 함수로 학습과정을 세밀하게 조정하거나, 중간 경과를 살펴볼 수 있음\n",
    "\n",
    "## 콜백 함수\n",
    "\n",
    "- 콜백함수 정의하기\n",
    "- keras.callbacks.Callback을 상속받아서 클래스 정의\n",
    "- 호출될 타이밍에 따라 해당 함수를 재정의\n",
    "  - on_train_end(self, logs=None): 학습이 종료될 때 호출\n",
    "  - on_epoch_end(self, epoch, logs=None): 한 epoch이 끝날 때 호출\n",
    "  - op_predict_end(self, logs=None):예측이 끝날 때 호출\n",
    "\n",
    "- 정의한 클래스의 인스턴스를 리스트에 포함시켜 callbacks에 전달\n",
    "\n",
    "```python\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    keys = list(logs.keys())\n",
    "    print(f\"End epoch {epoch} of training; got log keys: {keys}\")\n",
    "\n",
    "model.fit( ... , callbacks=[MyCallback()])\n",
    "```\n",
    "\n",
    "- 내장 콜백함수\n",
    "  - Tensorflow는 다양한 콜백함수를 제공\n",
    "  - tf.keras.callbacks 에 포함된 주요 콜백함수\n",
    "    - EarlyStopping: 학습이 진전이 없을 경우 조기에 학습을 종류\n",
    "    - ModelCheckpoint: 모델을 주기적으로 자동 저장\n",
    "    - TensorBoard: 학습 과정이나 모델의 정보를 시각화 할 수 있도록 로그를 기록\n",
    "\n",
    "### EarlyStopping\n",
    "\n",
    "```python\n",
    "tf.keras.callback.EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0, patience=0, verbose=0,\n",
    "  mode='auto', baseline=None, restore_best_weights=False\n",
    ")\n",
    "```\n",
    "\n",
    "- EarlyStopping 의 개념\n",
    "  - 일정 시점부터 학습 성능이 증가하지만, 검증 성능이 감소하거나 유지되는 과적합\n",
    "  - 과적합이 시작되면 더 학습을 징행할수록 성능이 감소함\n",
    "  - 과적합이 발생하는 Epoch를 감지하고 자동으로 학습을 종료하는 방법\n",
    "\n",
    "  - 모델 학습의 목표는 Loss값을 최소화 하는 과정\n",
    "  - Loss값을 감시하고 있다가 더 이상 감소하지 않는다명 학습이 종료됨\n",
    "  - 과적합을 방지하고 무의미한 학습과정을 생략하여 연산자원을 절약\n",
    "\n",
    "- monitor : 감지할 수치의 이름\n",
    "- mode : 원하는 방향\n",
    "  - monitor 값이 어떻게 되어야 하는 값인지 결정\n",
    "  - 'min': monitor 값이 최서가 되어야 성능이 좋아지는 모델일 때\n",
    "  - 'max': monitor 값이 최대가 되어야 성능이 좋아지는 모델일 때\n",
    "\n",
    "- patience : 종료 시점, 바로 종료할지, 몇번 더 학습 해볼것인지\n",
    "\n",
    "### ModelCheckpoint\n",
    "\n",
    "```python\n",
    "tf.keras.callback.ModelCheckpoint(\n",
    "  filepath, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "  save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "  options=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- ModelCheckpoint\n",
    "  - 일정 주기마다 모델이나 모델의 가중치를 자동으로 저장\n",
    "  - 과적합이 발생했을 때 처음부터 수행하지 않고 중간부터 다시 학습이 가능\n",
    "  - 선을 그래프를 보면서 가장 좋은 성능의 가중치를 서비스에 사용\n",
    "  - 가장 성능이 좋은 버전만 저장하는 기능이 포함\n",
    "\n",
    "- monitor, mode\n",
    "  - 감지할 지표와 모드: EarlyStopping과 동일\n",
    "\n",
    "- save_best_only : 가장 좋은 버전만 저장\n",
    "  - True: 성능이 가장 좋은 버전만 남기고, 다른 버전 모두 삭제\n",
    "  - False: 모든 버전을 저장\n",
    "- save_weights_only: 모델의 가중치의 값만 저장\n",
    "  - True :학습 진행상황, 모델의 구조에 대한 데이터를 빼고 저장하여 공간이 절약되지만 불러오기 위해서는 모델의 구조를 저장하고 있어야함(False 추천)\n",
    "\n",
    "- ModelCheckpoint의 주의할 점\n",
    "  - filepath: 체크 포인트를 저장할 디렉토리\n",
    "    - 고정된 문자열을 전달하면 하나의 디렉토리에 계속 저장\n",
    "    - Epoch에 따라 이름을 다르게 하도록 저장\n",
    "    - 포맷 문자열을 사용\n",
    "      - `filepath = 'checkpoints/cp-{epoch:04d}.ckpt`\n",
    "\n",
    "### TensorBoard\n",
    "\n",
    "- 모델의 정보를 시각화하는 TensorBoard를 사용할 수 있도록 로그를 기록\n",
    "  - 실시간 학습상황\n",
    "  - 모델의 구조\n",
    "\n",
    "- 장점\n",
    "  - 원격에서 실시간으로 모델의 학습정보를 포함한 정보를 확인\n",
    "  - 간단하게 Callback을 추구하고 시각화된 웹페이지를 제공\n",
    "  - 포트와 IP를 설정하면 서버가 아닌 원격에서 이 결과물을 활용 가능\n",
    "- 사용과정\n",
    "  - Callback에 TensorBoard 콜백함수를 추가하여 로그 디렉토리에 기록을 저장\n",
    "  - TensorBoard 모듈을 실행하여 로그 디렉토리의 로그를 웹페이지로 호스트\n",
    "  - 웹페이지에 출력된 정보들을 보고 학습 과정을 모니터링\n",
    "\n",
    "- TensorBoard 콜백함수의 인자\n",
    "  - log_dir : 로그를 저장할 경로\n",
    "  - update_freq : 저장하는 주기\n",
    "    - 'epoch' : 한 epoch마다 기록을 저장\n",
    "    - 'batch' : batch마다 기록을 저장(학습 속도가 느려짐)\n",
    "\n",
    "```python\n",
    "tf.keras.callbacks.TensorBoard(\n",
    "  log_dir='logs', histogram_freq=0, write_graph=True,\n",
    "  write_images=False, write_steps_per_second=False,\n",
    "  update_freq='epoch', profile_batch=0, embeddings_freq=0,\n",
    "  embeddings_metadata=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- Fit 함수에 콜백함수 추가하기\n",
    "```python\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n",
    "```\n",
    "\n",
    "- 명령줄에서 tensorboard를 실행\n",
    "  - '--logdir'에 저장한 디렉토리 경로를 전달\n",
    "```bash\n",
    "tensorboard --logdir=path_toyour_logs\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 서비스 하기\n",
    "\n",
    "## 모델 저장하고 불러오기\n",
    "\n",
    "- 모델의 구성요소\n",
    "  - 모델의 구조\n",
    "    - 레이어의 종류와 형태\n",
    "    - 입력 값의 형태\n",
    "  - 가중치 값\n",
    "    - 각 레이어의 행렬에 저장된 실제 float32 실수 값들\n",
    "    - 모델의 학습 = Loss값이 낮아직도록 가중치의 값을 수정하는 과정의 연속\n",
    "    - 같은 모델도 가중치 값에 따라 성능이 달라짐\n",
    "  - Compile 정보\n",
    "    - Optimizer의 종류, Learning Rate(lr), 사용한 Loss Function 정보\n",
    "\n",
    "- 모델의 저장 형식\n",
    "  - H5 Format\n",
    "    - 과거 Keras에서 사용하던 저장 방식\n",
    "    - 모델의 구조와 가중치를 포함한 정보들을 저장\n",
    "    - 사용자 정의 레이어와 손실함수는 저장하지 않음\n",
    "\n",
    "- SavedModel\n",
    "  - 최근에 사용하는 Tensorflow 표준 저장 형식\n",
    "  - 모델의 구조와 가중치를 포함한 정보들을 저장\n",
    "  - 사용자 정의 레이어와 손실함수까지 모두 저장\n",
    "\n",
    "- SavedModels 활용 방법\n",
    "  - 모델 저장\n",
    "\n",
    "```python\n",
    "model.fit(x, y)\n",
    "#'my_model'이라는 이름의 savedModel 폴더를 생성\n",
    "model.save('my_model')\n",
    "```\n",
    "\n",
    "  - 저장된 모델 사용\n",
    "\n",
    "```python\n",
    "#'my_model'이라는 이름의 savedModel 폴더를 생성\n",
    "loaded_model = keras.models.load_model('my_model')\n",
    "```\n",
    "\n",
    "- 이어서 학습하기\n",
    "  - SavedModel 형식은 모델의 모든 정보를 저장하고 불러오는 방식\n",
    "  - 불러온 모델을 그대로 학습을 마저 진행\n",
    "    - initial_epoch과 epoch을 조장하여 이어서 학습을 진행\n",
    "\n",
    "```python\n",
    "model.comile(...)\n",
    "model.fit(x, y)\n",
    "# 모델을 compile하고 학습까지 진행하고 저장\n",
    "model.save('my_model')\n",
    "\n",
    "loaded_model = keras.models.load_model('my_model')\n",
    "# 저장한 시점의 compile 정보와 학습 상태까지 불러옴\n",
    "# 다시 compile 하지 않아도 바로 이어서 학습 가능\n",
    "# 21 epoch부터 40 epoch 까지 계속 학습\n",
    "loaded_model.fit(x, y, initial_epoch = 20, epochs = 40)\n",
    "```\n",
    "\n",
    "- Checkpoint 불러오기\n",
    "  - Checkpoint 콜백함수의 인수 중 save_weights_only를 False로 설정\n",
    "  - 원하는 epoch의 체크포인트 경로를 전달\n",
    "  - initial_epoch과 epoch을 조절하여 이어서 학습을 진행\n",
    "\n",
    "```python\n",
    "# 모델을 학습하면서 Checkpoint 를 저장\n",
    "# 20epoch 이후 저장된 체크포인트를 불러옴\n",
    "loaded_model = keras.models.load_model(\"checkpoints/cp-0020.ckpt\")\n",
    "# 21 epoch부터 20 epoch까지 계속 학습\n",
    "loaded_model.fit(x, y, initial_epoch = 20, epochs = 40)\n",
    "```\n",
    "\n",
    "## 모델 서비스 방법\n",
    "\n",
    "- 자바스크립트를 이용한 모델 서비스\n",
    "  - Tensorflow.js\n",
    "    - 자바스크립트 Tensorflow 라이브러리\n",
    "    - 브라우저 또는 Node.js 에서 학습된 모델을 사용가능\n",
    "  - Tensorflow.js에서 사용하는 과정\n",
    "    - 모델의 학습을 Tensorflow에서 진행\n",
    "    - Python 에서 학습된 모델을 Tensorflow.js에 맞는 형식으로 변환\n",
    "    - Tensorflow.js 에서 모델의 동작을 js로 작성하여 서비스\n",
    "\n",
    "- Tensorflow.js 형식으로 변환하는 방법\n",
    "  - tensorflowjs 를 사용하여 변환하여 저장\n",
    "  - target_dir에 json파일을 포함한 변환된 모델이 저장됨\n",
    "\n",
    "```python\n",
    "import tensorflowjs as tfjs\n",
    "def train(...):\n",
    "  model = keras.models.Sequential()\n",
    "  ...\n",
    "  model.compile(...)\n",
    "  model.fit(...)\n",
    "\n",
    "  tfjs.converters.save_keras_model(model, target_dir)\n",
    "```\n",
    "\n",
    "- Tensorflowjs로 변환한 모델 사용방법\n",
    "  - model.json 파일의 URL을 제공하여 TensorFlow.js에 모델을 로드\n",
    "\n",
    "```js\n",
    "import * as tf from '@tensorflow/tfjs';\n",
    "const model = swait\n",
    "tf.loadLayersModel(\"https://foo.bar/tfjs_artifacts/model.json\")\n",
    "```\n",
    "\n",
    "- 모델을 학습, 추론하기\n",
    "\n",
    "```js\n",
    "const example = tf.fromPixels(webcamElement);  // 웹캠 Element를 사용한다고 가정\n",
    "const prediction = model.predict(example);     // 예측에 사용\n",
    "```\n",
    "\n",
    "- Flask에서 서비스하기\n",
    "  - 학습한 모델을 불러오기 : SavedModel 형식의 모델을 로드\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "  model = tf.keras.models.load_model(\"my_model\")\n",
    "  app.run(host='localhost', port=8000)\n",
    "```\n",
    "\n",
    "  - 모델의 사용은 그대로 predict를 사용\n",
    "\n",
    "```python\n",
    "res = model.predict([inputdata])\n",
    "# res에서 정보를 추출하여 서비스에 활용\n",
    "```\n",
    "\n",
    "## 서버 안정화 처리\n",
    "\n",
    "- 딥러닝 모델 서비스\n",
    "  - 모델의 추론과정은 일반적인 연산에 비해 처리 시간이 길고 자원도 많이 소모\n",
    "  - GPU자원을 사용하는 경우 GPU를 병렬적으로 사용하기 위한 처리가 필요\n",
    "  - 사용자의 요청에 따라 계속 연산을 처리하면 서비스가 종료될 수 있음\n",
    "\n",
    "- 서비스 안정화\n",
    "  - 사용자의 요청을 거절하더라도 서비스가 종료되지 않도록 자원관리가 필요\n",
    "  - 서버의 연산 성능, 처리중인 작업의 수를 고려한 설계가 요구됨\n",
    "\n",
    "- 실행 가능한 작업 제한\n",
    "  - 서버에서 동시에 진행가능한 작업의 수를 제한하여 안정화\n",
    "  - 구현 방법의 예시\n",
    "    - 동시에 처리 가능한 최대 작업의 수를 상수로 정의(max_works)\n",
    "    - 전역변수를 이용하여 진행중인 작업의 수를 저장(num_works)\n",
    "    - 사용자의 요청이 왔을 때, 진행중인 작업의 수 비교(max_works > num_works)\n",
    "      - 작업의 수가 최대일 때 : 작업을 수락하고 num_works를 1 증가 시킴\n",
    "      - 작업의 수가 최대가 아닐 때 : 작업을 수락하고 num_works를 1 증가시킴\n",
    "    - 작업이 완료되면 작업 수를 1 감소(num_works -= 1)\n",
    "\n",
    "  - 장점\n",
    "    - 비교적 간단하게 구현이 가능\n",
    "    - 가벼운 모델만 사용할 때, 서비스가 다운되는 현상은 방지할 수 있음\n",
    "  - 단점\n",
    "    - 사용자가 작업을 예약하는 등의 처리는 어려움\n",
    "    - 작업이 매우 오래 걸리는 모델의 경우 응답까지 시간이 오래걸림\n",
    "    - 대규모 서비스에 작용하기는 부적절함\n",
    "\n",
    "- 작업 큐를 이용한 비동기 처리\n",
    "  - 사용자와 상호작용을 관리하는 프로세스와 작업을 처리하는 Worker프로세스를 분리\n",
    "  - 각 프로세스는 큐를 이용하여 상호작용함\n",
    "  - 유사한 방식이 활용되는 분야\n",
    "    - 안드로이드 GUI의 이벤트 시스템\n",
    "    - Windows의 메시지 큐와 윈도우 프로시저\n",
    "    - 그래픽 처리 분야의 렌더링 큐\n",
    "    - 프린터의 스풀링작업\n",
    "    - Tensorflow의 데이터를 불러오기 위한 workers 옵션\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
