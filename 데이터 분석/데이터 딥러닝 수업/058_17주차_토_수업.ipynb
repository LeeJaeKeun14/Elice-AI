{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기초\n",
    "\n",
    "## 딥러닝 모델 학습의 문제점\n",
    "\n",
    "1. 학습 속도 문제\n",
    "   - 데이터의 개수가 폭발적으로 증가하여 딥러닝 모델 학습 시 소요되는 시간도 함께 증가\n",
    "2. 기울기 소실 문제\n",
    "   - 더 깊고 더 넓은 망을 학습시키는 과정에서 출력값과 멀어질수록 학습이 잘 안되는 현상 발생\n",
    "3. 초기값 설정 문제\n",
    "   - 초기값 설정 방식에 따른 성능 차이가 매우 크게 발생\n",
    "4. 과적합 문제\n",
    "   - 학습 데이터에 모델이 과하게 과적화되어 테스트 성능 저하 \n",
    "\n",
    "## 학습 속도 문제와 최적화 알고리즘\n",
    "\n",
    "### SGD\n",
    "\n",
    "- 학습 속도 문제의 발생 원인\n",
    "  - 전체 학습 데이터 셋을 사용하여 손실 함수를 계산하기 때문에 계산량이 너무 많아짐\n",
    "  - 전체 데이터가 아닌 부분 데이터만 활용하여 손실 함수 계산\n",
    "  - SGD(stochastic Gradient Descent)\n",
    "    - 전체 데이터(batch) 대신 일부 조그마한 데이터의 모음인 미니배치에 대해서만 손실 함수를 계산\n",
    "\n",
    "- 빠른 시간에 더 많이 학습하는 SGD알고리즘\n",
    "  - 다소 부정확할 수 있지만, 훨씬 계산 속도가 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있음\n",
    "  - mini-batch에 따라 gradient 방향의 변화가 큼\n",
    "  - SGD의 한계 : Learning Rate 설정 문제\n",
    "    - Gradient 방향 해결방안 : Momentum\n",
    "    - Learning rate 해결방안 : Adagrad\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- 과거에 이동했던 방식을 기억하면서 그 방향으로 일정 정도를 추가적으로 이동하는 방식\n",
    "\n",
    "### AdaGrad(Adaptive Gradient)\n",
    "\n",
    "- 많이 변화하지 않은 변수들은 Learning rate를 크게하고,\n",
    "- 많이 변화했던 변수들은 Learning rate를 작게 하는것\n",
    "\n",
    "- 과거의 기울기를 제곱해서 계속 더하기 때문에\n",
    "  - 학습이 진행될수록 갱신 강도가 약해짐\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "- 무한히 학습하다 보면 순간 갱신량이 0에 가까워 학습이 되지 않는 Adagrad의 단점을 해결\n",
    "- 과거의 기울기는 잊고 새로은 기울기 정보를 크게 반영\n",
    "\n",
    "### Adam\n",
    "\n",
    "-  Momentum + RMSProp\n",
    "\n",
    "## 딥러닝 모델 학습 방법 되짚어보기\n",
    "\n",
    "- 나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘\n",
    "\n",
    "- 기울기 소실 문제(Vanishing Gradient)의 발생 원인\n",
    "  - 기울기가 0인 값을 전달하며 중간 전달값이 사라지는 문제\n",
    "  - 기울기가 소실되는 문제가 반복되며 학습이 잘 이루어지지 않음\n",
    "\n",
    "- 기울기 소실 문제 해결 방법 - ReLU\n",
    "  - 활성화 함수(Activation Function) 방식 변화\n",
    "  - 기존에 사용하던 sigmoid 함수 대신 ReLU 함수를 사용하여 해결\n",
    "\n",
    "  - Tanh\n",
    "  - 내부 Hidden Layer에는 ReLu를 적용하고\n",
    "  - Output Layer에서만 Tanh를 적용\n",
    "\n",
    "## 초기값 설정 문제와 방지 기법\n",
    "\n",
    "### 잘못된 초기값 설정 - 초기화의 중요성\n",
    "\n",
    "- 가중치 초기화(Weight Initialization)\n",
    "  - 활성화 함수의 입력 값이 너무 커지거나\n",
    "  - 작아지지 않게 만들어주려는 것이 핵심\n",
    "\n",
    "- 초기화 설정 문제 해결을 위한 Naive 한 방법\n",
    "  - 표준 정규분포를 이용해 초기화\n",
    "  - 표준편차를 0.01로 하는 정규분포로 초기화\n",
    "\n",
    "### Xavier 초기화 방법 + Sigmoid 함수\n",
    "\n",
    "- 표준 정규 분포를 입력 개수의 제곱근으로 나누어 줌\n",
    "- Sigmoid와 같은 S자 함수의 경우 출력 값들이 정규푼포형태를 가져야 안정적으로 학습 가능\n",
    "\n",
    "- ReLU 함수에는 Xavier 초기화가 부적합\n",
    "  - 레이어를 거쳐갈수록 값이 0에 수렴\n",
    "\n",
    "### He 초기화 방법 + ReLU 함수\n",
    "\n",
    "- 표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어줌\n",
    "- ReLU 함수와 HE 초기화 방법을 사용했을 경우 그래프는 위와같음(0 중심 분포)\n",
    "- 10층 레이어에서도 평균과 표준편차가 0으로 수렴하지 않음 \n",
    "\n",
    "- 적절한 가중치 초기화 방법\n",
    "  - Sigmoid, tanh의 경우 Xavier 초기화 방법이 효율적\n",
    "  - ReLU 경우 He 초기화 방법이 효율적\n",
    "\n",
    "## 과적합 문제와 방지 기법\n",
    "\n",
    "### 딥러닝 모델 학습에서의 과적합 방지 기법\n",
    "\n",
    "- 정규화(Regularization)\n",
    "- 드롭아웃(Dropout)\n",
    "- 배치 정규화(Batch Normalization)\n",
    "\n",
    "### 정규화\n",
    "\n",
    "- 모델이 복잡해질수록 Parameter들은 많아지고, 절댓값이 커지는 경향이 발생함\n",
    "- 기존 손실함수에 규제항을 저해 최적값 찾기 가능\n",
    "  - L1 정규화(Lasso Regularization)\n",
    "    - 가중치의 절댓값의 합을 규제항으로 정의\n",
    "    - 작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음\n",
    "  - L2 정규화(Ridge Regularization)\n",
    "    - 가중치의 제곱의 합을 규제항으로 정의\n",
    "    - L1정규화에 비하여 0으로 수렴하는 가중치가 적음, 큰값을 가진 가중치를 더욱 제약하는 효과\n",
    "\n",
    "### 드롭아웃\n",
    "\n",
    "- 각 layer마다 일정 비율의 뉴런을 임의로 dropp시켜 나머지 뉴런들만 학습하는 방법\n",
    "- 드롭아웃을 적용하면 학습되는 노드와 가중치들이 매번 달라짐\n",
    "\n",
    "  - 다른 정규화 기법들과 상호 보완적으로 사용 가능\n",
    "  - drop된 뉴런은 backpropagation때 신호를 차단\n",
    "  - test때는 모든 뉴런에 신호를 전달\n",
    "\n",
    "### 배치 정규화\n",
    "\n",
    "- Normalization을 처음 input data 뿐만 아니라 신경망 내부 Hidden Layer의 input에도 사용\n",
    "  - 매 Layer마다 정규화를 진행하므로 가중치 초기값에 크게 의존하지 않음\n",
    "  - 과적합 억제\n",
    "  - 학습 속도의 향상\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
