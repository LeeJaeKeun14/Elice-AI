{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 심화\n",
    "\n",
    "- 회귀분석 \n",
    "  1. 회귀 개념 알아보기\n",
    "  2. 단순 선형회귀\n",
    "  3. 다중 선형회귀와 다항회귀\n",
    "  4. 과적합과 정규화\n",
    "  5. 정규화를 적용한 회귀\n",
    "  6. 회귀 알고리즘 평가 지표\n",
    "- 분류 개념\n",
    "  1. 로지스틱 회귀\n",
    "  2. Support Vector Machine(SVM)\n",
    "  3. KNN(k-nearest Neighbor)\n",
    "  4. 나이브 베이즈 분류\n",
    "  5. 분류 알고리즘 평가 지표(1)\n",
    "  6. 분류 알고리즘 평가 지표(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 개념\n",
    "\n",
    "- 회귀분석\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X $$\n",
    "\n",
    "- 적절한 $\\beta_0, \\beta_1$ 찾기\n",
    "  - Loss function을 최소화 하는 값 찾기\n",
    "\n",
    "- 경사하강법\n",
    "  - Gradient Descent\n",
    "\n",
    "## 단순 선형 회귀\n",
    "\n",
    "- $ Y = \\beta_0 + \\beta_1X $\n",
    "\n",
    "## 다중 선형회귀와 다항회귀\n",
    "\n",
    "- 다중 선형회귀(Multiple Linear Regression)\n",
    "  - $ Y = \\beta_0 + \\beta_1X + ... + \\beta_iX_i$\n",
    "  - 여러 개의 입력값과 결과값 간의 관계 확인 가능\n",
    "  - 어떤 입력값이 결과값에 어떠한 영향을 미치는 지 않 수 있음.\n",
    "  - 여러 개의 입력값 사이 간의 상관 관계가 높을 경우 결과에 대한 신뢰성을 잃을 가능성이 있음.\n",
    "\n",
    "- 다항 회귀(Polynomial Regression)\n",
    "  - $ Y = \\beta_0 + \\beta_1X + ... + \\beta_iX_i^i$\n",
    "  - 일차 함수 식으로 표현할 수 없는 복잡한 데이터 분포에도 적용 가능\n",
    "  - 극단적으로 높은 차수의 모델을 구현할 경우 과도하게 학습 데이터에 맞춰지는 과적합 현상 발생\n",
    "  - 데이터 관계를 선형으로 표현하기 어려운 경우 사용\n",
    "\n",
    "## 과적합과 정규화\n",
    "\n",
    "- Overfitting\n",
    "  - 모델이 주어진 훈련 데이터에 과도하게 맞춰져 새로운 데이터가 입력 되었을 때 잘 예측하지 못하는 현상 \n",
    "  - 즉 모델이 과도하게 복잡해져 일반성이 떨어진 경우를 의미함\n",
    "\n",
    "- 과적합 방지 방법\n",
    "  - 모델이 잘 적합되어 실제 데이터와 유사한 예측 결과를 얻을 수 있도록 과적합 방지를 위해 다양한 방법을 사용함\n",
    "\n",
    "- 교차검증(Cross Validation) & 정규화(Regularization)\n",
    "  - 교차검증\n",
    "    - 모델이 잘 적합되는지 알아보기 위해 훈련용 데이터와 별개의 테스트 데이터, 그리고 검증 데이터로 나누어 성능 평가하는 방법\n",
    "    - 다양한 방법들이 있지만, 일반적으로 K-fold 교차검증을 많이 사용함\n",
    "\n",
    "- 정규화\n",
    "  - 모델의 복잡성을 줄여 일반화된 모델을 구현하기 위한 방법\n",
    "  - L1 정규화(Lasso)\n",
    "    - 불필요한 입력값에 대응되는 $\\beta_i$를 정확히 0으로 만든다\n",
    "  - L2 정규화(Ridge)\n",
    "    - 아주 큰 값이나 작은 값을 가지는 이상치에 대한 $\\beta_i$를 0에 가까운 값으로 만든다\n",
    "\n",
    "## 정규화를 적용한 회귀\n",
    "\n",
    "- Lasso Regression\n",
    "  - 회귀 학습에 사용되는 Loss Function(비용함수)에 L1정규화 항을 추가\n",
    "  - 중요하지 않은 $\\beta$를 0으로 만들어 모델의 복잡성을 줄일 수 있음\n",
    "    - 너무 많은 $\\beta$를 0으로 만들 수 있어 모델의 정확성이 떨어질 수 있음\n",
    "    - 몇 개의 중요 변수만 선택하기 때문에 정보손실의 가능성이 있음\n",
    "\n",
    "- Ridge Regression\n",
    "  - 회귀 학습에 사용되는 Loss Function(비용함수)에 L2정규화 항을 추가\n",
    "  - 중요하지 않은 $\\beta$를 0게 가갑게 만들어 모델의 복잡성을 줄일 수 있음\n",
    "    - $\\beta$를 0에 가깝게 만들었지만 완전한 0은 아니기 때문에 모델이 여전히 복잡할 수 있음\n",
    "\n",
    "## 회귀 알고리즘 평가 지표\n",
    "\n",
    "- 회귀 알고리즘 평가\n",
    "  - 어떤 모델이 좋은 모델인지 평가 지표\n",
    "    - RSS, MSE, MAE, MAPE, $R^2$\n",
    "\n",
    "- RSS : 단순 오차\n",
    "  - Residual Sume of Squares\n",
    "    1. 실제값과 예측값의 단순 오차 제곱 합\n",
    "    2. 값이 작을수록 모델의 성능이 높음\n",
    "    3. 전체 데이터에 대한 실제 값과 예측하는 값의 오차 제곱의 총합\n",
    "  - 가장 간단한 평가 방법으로 직관적인 해석이 가능함\n",
    "  - 그러나 오차를 그대로 이용하기 때문에 입력값의 크기에 의존적임\n",
    "\n",
    "- MSE, MAE - 절대적인 크기에 의존한 지표\n",
    "  - MSE(Mean Squared Error)\n",
    "    - 평균 제곱 오차, RSS 에서 데이터 수만큼 나눈 값\n",
    "    - 작을 수록 모델의 성능이 높다고 평가할 수 있음\n",
    "  - MAE(Mean Absolute Error)\n",
    "    - 평균 절대값 오차, 실제값과 예측값의 오차의 절대값의 평균\n",
    "    - 작을 수록 모델의 성능이 높다고 평가할 수 있음.\n",
    "  - MSE : 이상치(Outlier)즉, 데이터들 중 크게 떨어진 값에 민감함\n",
    "  - MAE : 변동성이 큰 지표와 낮은 지표를 같이 예측할 시 유용\n",
    "    - 가장 간단한 평가 방법들로 직관적인 해석이 가능함\n",
    "    - 그러나 평균을 그대로 이용하기 때문에 입력값이 크기에 위존적임\n",
    "\n",
    "- $R^2$ : 결정 계수\n",
    "  - 회귀 모델의 설명력을 표현하는 지표\n",
    "  - 1에 가까울 수록 높은 성능의 모델이라고 해석할 수 있음\n",
    "    $$ 1 - \\frac{RSS}{TSS} \\;\\; (0 \\leq R^2 \\leq 1)  $$\n",
    "  - 백분율로 표현하기 때문에 크기에 의존적이지 않음\n",
    "  - 실제값이 1보다 작을 경우, 무한대에 가까운 값 도출, 실제값이 0일 경우 계산 불가ㅅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류 \n",
    "\n",
    "## 분류 개념과 로지스틱 회귀\n",
    "\n",
    "일반적인 회귀 알고리즘은 분류 문제에 그대로 사용할 수 없다.\n",
    "\n",
    "- 선형 회귀는 $-\\infty \\sim +\\infty$ 의 값을 가질 수 있음\n",
    "- 팡별의 결과값은 0 ~ 1\n",
    "\n",
    "\n",
    "- 해당 클래스에 속할 확률인 0 또는 1 사이의 값만 가지는 시그모이드 함수 사용\n",
    "  - 이를 로지스틱 회귀 라고함\n",
    "\n",
    "$$ g(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x} $$\n",
    "\n",
    "- S 형 곡선을 갖는 시그모이드 함수\n",
    "  - x 값이 커질 경우 g(x) 값은 점점 1에 수렴하고\n",
    "  - x 값이 작아질 경우 g(x) 값은 점점 0에 수렴함\n",
    "\n",
    "- 주로 2개 값 분류(이진 분류)를 위해 사용\n",
    "- 선형 회귀를 응용한 분류 알고리즘 이기 때문에 선형 회귀의 특징을 보유\n",
    "\n",
    "## SVM\n",
    "\n",
    "딥러닝 기술 등장 이전까지 가장 인기 있던 분류 알고리즘.\n",
    "\n",
    "- 선형 분류와 비선형 분류 모두 가능\n",
    "- 고차원 데이터에서도 높은 성능의 결과를 도출\n",
    "- 회귀에도 적용 가능\n",
    "\n",
    "## 나이브 베이즈 분류\n",
    "\n",
    "- 각 특징들이 독립적 즉, 서로 영향을 미치지 않을 것이라는 가정 설정\n",
    "- 베이즈 정리(Bayes Rule)를 활용한 확률 통계학적 분류 알고리즘\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)} $$ \n",
    "\n",
    "- 베이즈 정리\n",
    "  - 각 특징들이 독립이라면 다른 분류 방식에 비해 결과가 좋고, 학습 데이터도 적게 필요\n",
    "  - 각 특징들이 독립이 아니라면 즉, 특징들이 서로 영향을 미치면 분류 결과 신회성 하락\n",
    "  - 학습 데이터에 없는 범주의 데이터일 경우 정상적 예측 불가능\n",
    "\n",
    "## KNN\n",
    "\n",
    "- 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론\n",
    "- 유사한 특성을 가진 데이터는 유사 범주에 속하는 경향이 있다는 가정 하에 분류\n",
    "\n",
    "- 직관적이며 복잡하지 않은 알고리즘, 결과 해석이 쉬움\n",
    "- K값 결정에 따라 성능이 크게 좌우됨\n",
    "- 딱히 학습이랄 것이 없는 Lazy Model\n",
    "\n",
    "## 분류 알고리즘 평가 지표(1)\n",
    "\n",
    "- 혼동 행렬(Confusion Matrix)\n",
    "  - 분류 모델의 성능을 평가하기 위함\n",
    "\n",
    "|      |          |          |     예측 |\n",
    "| :---:|   :---:  | :---:    |   :---:  |\n",
    "|      |          | Positive | Negative | \n",
    "|      | Positive |    TP    |    FN    | \n",
    "| 실제 | Negative |    FP    |    TN    |\n",
    "\n",
    "- TP(ture positive) : 실제 긍정값을 긍정으로 예측(정답)\n",
    "- TN(ture nogative) : 실제 부정값을 부정으로 예측(정답)\n",
    "\n",
    "- FP(false positive) : 실제 부정값을 긍정으로 예측(1형 오류)\n",
    "- FN(false negative) : 실제 긍정값을 부정으로 예측(1형 오류)\n",
    "\n",
    "- 정확도(Accuracy)\n",
    "\n",
    "accuracy = (TP + TN) / (P + N)\n",
    "\n",
    "- 정밀도(Precision)\n",
    "  - Negative가 중요한 경우\n",
    "    - Precision = TP / (TP + FP)\n",
    "\n",
    "- 재현율(Recall, TPR)\n",
    "  - Positive가 중요한 경우\n",
    "    - Recall = TP / (TP + FN) = TP / P\n",
    "\n",
    "- FPR(False Positive Rate)\n",
    "  - 실제 Negative인 데이터 중에서 모델이 Positive로 분류한 데이터 비율\n",
    "    - FPR = FP / (FP + TN) = FP / N\n",
    "    - 게임에서 비정상 사용자 검출 시 FPR이 높다.\n",
    "      - 정상 사용자와 비정상 사용자로 검출하는 경우가 많다\n",
    "    - 이 때 비정상 사용자에 대해 계정정지 등 패널치를 부여할 경우 선의의 사용자가 피해를 입게 될 가능성이 높음\n",
    "\n",
    "- ROC Curve와 AUC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
